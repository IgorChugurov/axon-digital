import {
  BedrockRuntimeClient,
  ConverseCommand,
  type ConverseCommandInput,
  type Message,
  type SystemContentBlock,
} from "@aws-sdk/client-bedrock-runtime";
import { LLMGenerationRequest, LLMGenerationResponse } from "../types";

export class LLMService {
  private client: BedrockRuntimeClient;
  private modelId: string;
  private lastRequestTime: number = 0;
  private minRequestInterval: number;
  private maxRetries: number;
  private baseRetryDelay: number;

  constructor() {
    const region = process.env.BEDROCK_REGION;
    if (!region) {
      throw new Error("BEDROCK_REGION is not configured");
    }

    this.modelId = process.env.BEDROCK_MODELID || "";
    if (!this.modelId) {
      throw new Error("BEDROCK_MODELID is not configured");
    }

    // Map your BEDROCK_API_KEY to the SDK's bearer token env for Bedrock
    if (!process.env.AWS_BEARER_TOKEN_BEDROCK && process.env.BEDROCK_API_KEY) {
      process.env.AWS_BEARER_TOKEN_BEDROCK = process.env.BEDROCK_API_KEY;
    }

    // Use default provider chain; with AWS_BEARER_TOKEN_BEDROCK set, SDK will send bearer token
    this.client = new BedrockRuntimeClient({ region });

    // –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è rate limiting –∏ retry
    this.minRequestInterval = parseInt(
      process.env.BEDROCK_MIN_INTERVAL || "1500"
    ); // 1.5 —Å–µ–∫—É–Ω–¥—ã –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é
    this.maxRetries = parseInt(process.env.BEDROCK_MAX_RETRIES || "3");
    this.baseRetryDelay = parseInt(
      process.env.BEDROCK_BASE_RETRY_DELAY || "2000"
    ); // 2 —Å–µ–∫—É–Ω–¥—ã
  }

  private async sleep(ms: number): Promise<void> {
    return new Promise((resolve) => setTimeout(resolve, ms));
  }

  private async enforceRateLimit(): Promise<void> {
    const now = Date.now();
    const timeSinceLastRequest = now - this.lastRequestTime;

    if (timeSinceLastRequest < this.minRequestInterval) {
      const waitTime = this.minRequestInterval - timeSinceLastRequest;
      console.log(
        `‚è±Ô∏è Rate limiting: waiting ${waitTime}ms before next request`
      );
      await this.sleep(waitTime);
    }

    this.lastRequestTime = Date.now();
  }

  private async retryWithBackoff<T>(
    operation: () => Promise<T>,
    maxRetries: number = 3,
    baseDelay: number = 1000
  ): Promise<T> {
    let lastError: Error;

    for (let attempt = 0; attempt <= maxRetries; attempt++) {
      try {
        return await operation();
      } catch (error) {
        lastError = error as Error;

        // –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ —ç—Ç–æ –∏–º–µ–Ω–Ω–æ ThrottlingException
        if (
          error instanceof Error &&
          error.message.includes("Too many requests")
        ) {
          if (attempt < maxRetries) {
            const delay = baseDelay * Math.pow(2, attempt); // –≠–∫—Å–ø–æ–Ω–µ–Ω—Ü–∏–∞–ª—å–Ω–∞—è –∑–∞–¥–µ—Ä–∂–∫–∞
            console.log(
              `üîÑ Retry attempt ${attempt + 1}/${maxRetries + 1} after ${delay}ms delay...`
            );
            await this.sleep(delay);
            continue;
          }
        }

        // –ï—Å–ª–∏ —ç—Ç–æ –Ω–µ throttling –æ—à–∏–±–∫–∞, —Å—Ä–∞–∑—É –≤—ã–±—Ä–∞—Å—ã–≤–∞–µ–º
        throw error;
      }
    }

    throw lastError!;
  }

  async generateResponse(
    request: LLMGenerationRequest
  ): Promise<LLMGenerationResponse> {
    return this.retryWithBackoff(
      async () => {
        try {
          // –ü—Ä–∏–º–µ–Ω—è–µ–º rate limiting
          await this.enforceRateLimit();

          const { prompt, body, projectId, documentName } = request;

          // –§–æ—Ä–º–∏—Ä—É–µ–º —Å–∏—Å—Ç–µ–º–Ω—ã–µ –±–ª–æ–∫–∏
          const systemBlocks: SystemContentBlock[] = [];
          let promptText = prompt;

          // –î–æ–±–∞–≤–ª—è–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –¥–æ–∫—É–º–µ–Ω—Ç–µ –µ—Å–ª–∏ –µ—Å—Ç—å
          // if (documentName) {
          //   promptText += `\n\nDocument: ${documentName}`;
          // }

          systemBlocks.push({ text: promptText });

          // –§–æ—Ä–º–∏—Ä—É–µ–º —Å–æ–æ–±—â–µ–Ω–∏—è
          const messages: Message[] = [
            {
              role: "user",
              content: [{ text: body }],
            },
          ];

          console.log("ü§ñ Calling Bedrock directly:", {
            modelId: this.modelId,
            projectId,
            documentName,
            promptLength: prompt.length,
            bodyLength: body.length,
          });

          const input: ConverseCommandInput = {
            modelId: this.modelId,
            messages,
            system: systemBlocks,
            inferenceConfig: {
              maxTokens: 4000,
              temperature: 0.7,
              topP: 0.9,
            },
          };

          const command = new ConverseCommand(input);
          const response = await this.client.send(command);

          // –ò–∑–≤–ª–µ–∫–∞–µ–º —Ç–µ–∫—Å—Ç –æ—Ç–≤–µ—Ç–∞
          const messageContent = response.output?.message?.content;
          let messageText = "";

          if (Array.isArray(messageContent) && messageContent.length > 0) {
            const firstContent = messageContent[0];
            if (
              firstContent &&
              typeof firstContent === "object" &&
              "text" in firstContent
            ) {
              messageText = String(firstContent.text);
            }
          }

          if (!messageText) {
            throw new Error(
              "Invalid response from Bedrock - missing message content"
            );
          }

          // –ò–∑–≤–ª–µ–∫–∞–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ —Ç–æ–∫–µ–Ω–∞—Ö
          const usage = response.usage;
          const tokenUsage = usage
            ? {
                inputTokens: usage.inputTokens || 0,
                outputTokens: usage.outputTokens || 0,
                totalTokens:
                  (usage.inputTokens || 0) + (usage.outputTokens || 0),
              }
            : undefined;

          console.log("‚úÖ LLM response generated directly:", {
            responseLength: messageText.length,
            projectId,
            tokenUsage,
          });

          return {
            response: messageText,
            usage: tokenUsage,
          };
        } catch (error) {
          console.error("‚ùå Direct LLM Service error:", error);
          throw new Error(
            error instanceof Error
              ? `Direct LLM generation failed: ${error.message}`
              : "Unknown direct LLM generation error"
          );
        }
      },
      this.maxRetries,
      this.baseRetryDelay
    );
  }

  async generateByProvider(
    projectId: string,
    request: LLMGenerationRequest
  ): Promise<LLMGenerationResponse> {
    // –í –±—É–¥—É—â–µ–º –∑–¥–µ—Å—å –º–æ–∂–Ω–æ –¥–æ–±–∞–≤–∏—Ç—å –ª–æ–≥–∏–∫—É –≤—ã–±–æ—Ä–∞ –ø—Ä–æ–≤–∞–π–¥–µ—Ä–∞ (BEDROCK, HATHR –∏ —Ç.–¥.)
    return this.generateResponse({
      ...request,
      projectId,
    });
  }
}

// –≠–∫—Å–ø–æ—Ä—Ç–∏—Ä—É–µ–º singleton instance
export const llmService = new LLMService();
